{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbi3soEqPyOhwzAuI5Z1/l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taylorfp99/Wine/blob/main/wandbml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "import wandb\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ExAQjwWHO8Sk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SD3GzagpLH-4"
      },
      "outputs": [],
      "source": [
        "wine=load_wine()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X,y=wine.data, wine.target"
      ],
      "metadata": {
        "id": "nXWacMAoLgck"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=357)"
      ],
      "metadata": {
        "id": "d9wjYOcxOVu7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    'learning_rates': [0.01, 0.1, 0.2, 0.25],\n",
        "    'max_depths': [2, 3, 4, 5],\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'loss_functions': ['deviance'],  # otras son log_loss, squared_error\n",
        "    'subsamples': [0.8, 1.0],  # una tasa mas baja previene el sobreajuste pero influye en la varianza del modelo\n",
        "    'min_samples_splits': [2, 4],\n",
        "    'min_samples_leafs': [1, 2]\n",
        "}\n",
        "\n",
        "for lr in hyperparameters['learning_rates']:\n",
        "    for max_depth in hyperparameters['max_depths']:\n",
        "        for n_estimator in hyperparameters['n_estimators']:\n",
        "            for loss_function in hyperparameters['loss_functions']:\n",
        "                for subsample in hyperparameters['subsamples']:\n",
        "                    for min_samples_split in hyperparameters['min_samples_splits']:\n",
        "                        for min_samples_leaf in hyperparameters['min_samples_leafs']:\n",
        "                            experiment_name = f\"gbm_lr{lr}_depth{max_depth}_est{n_estimator}_loss{loss_function}_subsample{subsample}_minsplit{min_samples_split}_minleaf{min_samples_leaf}\"\n",
        "                            wandb.init(project=\"vinito-upgrade\", name=experiment_name, config=hyperparameters)\n",
        "\n",
        "                            clf = GradientBoostingClassifier(learning_rate=lr, max_depth=max_depth, n_estimators=n_estimator,\n",
        "                                                             loss=loss_function, subsample=subsample, min_samples_split=min_samples_split,\n",
        "                                                             min_samples_leaf=min_samples_leaf, random_state=357, validation_fraction=0.1,\n",
        "                                                             n_iter_no_change=5, tol=0.01)\n",
        "\n",
        "                            clf.fit(X_train, y_train)\n",
        "\n",
        "                            # Make predictions\n",
        "                            y_pred = clf.predict(X_test)\n",
        "                            y_pred_proba = clf.predict_proba(X_test)\n",
        "\n",
        "                            # Log hyperparameters to wandb\n",
        "                            wandb.config.learning_rate = lr\n",
        "                            wandb.config.max_depth = max_depth\n",
        "                            wandb.config.n_estimators = n_estimator\n",
        "                            wandb.config.loss_function = loss_function\n",
        "                            wandb.config.subsample = subsample\n",
        "                            wandb.config.min_samples_split = min_samples_split\n",
        "                            wandb.config.min_samples_leaf = min_samples_leaf\n",
        "\n",
        "                            # Calculate performance metrics\n",
        "                            accuracy = accuracy_score(y_test, y_pred)\n",
        "                            f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
        "                            y_test_bin = label_binarize(y_test, classes=np.unique(y))\n",
        "                            y_pred_bin = y_pred_proba.reshape(-1, len(np.unique(y)))\n",
        "                            roc_auc_macro = roc_auc_score(y_test_bin, y_pred_bin, average=\"macro\", multi_class=\"ovr\")\n",
        "\n",
        "                            # Log metrics to wandb\n",
        "                            wandb.log({\"accuracy\": accuracy, \"f1_macro\": f1_macro, \"roc_auc_macro\": roc_auc_macro,\n",
        "                                       \"validation_score\": clf.train_score_[-1]})\n",
        "\n",
        "                            # Finish the experiment\n",
        "                            wandb.finish()"
      ],
      "metadata": {
        "id": "5KXDVz5XgB2u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}